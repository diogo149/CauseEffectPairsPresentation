{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "import warnings\n",
      "import time\n",
      "\n",
      "warnings.simplefilter(\"ignore\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVR\n",
      "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
      "from sklearn.metrics import auc_score\n",
      "from sklearn.preprocessing import OneHotEncoder"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cause-Effect Pairs\n",
      "--------------\n",
      "![](http://kaggle2.blob.core.windows.net/competitions/kaggle/3370/logos/front_page.png \"cep\")\n",
      "\n",
      "http://www.kaggle.com/c/cause-effect-pairs/\n",
      "\n",
      "### Goals:\n",
      "+ #### Introduction to the challenge\n",
      "+ #### Philosophy behind my solution\n",
      "+ #### Rough overview of my solution\n",
      "+ #### Possible improvements"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Understanding The Problem\n",
      "----------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### \"Given samples from a pair of variables A, B, find whether A is a cause of B.\"\n",
      "\n",
      "+ #### The objective of the challenge is to rank pairs of variables {A, B} to prioritize experimental verifications of the conjecture that A causes B.\n",
      "+ #### This challenge is limited to pairs of variables deprived of their context.\n",
      "+ #### For the training data, we are given whether\n",
      "    + A causes B\n",
      "    + B cases A\n",
      "    + A and B are consequences of a common cause, or \n",
      "    + A and B are indepednent.\n",
      "+ #### Error Metric: Bi-directional AUC -> It's a ranking problem.\n",
      "+ #### Simplified: Is it more likely that y = f(x, noise) or x = f(y, noise)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bidirectional_auc(true, predictions):\n",
      "    return (auc_score(true == 1, predictions) + auc_score(true == -1, -predictions)) / 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import Image\n",
      "Image('quiz.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Understanding The Data\n",
      "------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "folder = \"SUP2data_text\"  # Using SUP2 because SUP1 and SUP3 have no categorical data!!!\n",
      "pairs = pd.read_csv(os.path.join(folder, \"CEdata_train_pairs.csv\"))\n",
      "target = pd.read_csv(os.path.join(folder, \"CEdata_train_target.csv\"))\n",
      "publicinfo = pd.read_csv(os.path.join(folder, \"CEdata_train_publicinfo.csv\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# utility functions!\n",
      "\n",
      "def to_array(pair_str):\n",
      "    return np.array(map(int, pair_str.split()))\n",
      "\n",
      "def get_pair(idx, df):\n",
      "    return np.array(df['A'])[idx], np.array(df['B'])[idx]\n",
      "\n",
      "def plot_pair(idx, subset=(pairs, target)):\n",
      "    df, target = subset\n",
      "    x, y = map(to_array, get_pair(idx, df))\n",
      "    print(target.iloc[idx])\n",
      "    plot(x, y, 'o')\n",
      "    xlabel('A')\n",
      "    ylabel('B')\n",
      "    pylab.show()\n",
      "    return x, y\n",
      "    \n",
      "def type_finder(a_type, b_type):\n",
      "    idxs = (a_type == publicinfo['A type']) * (b_type == publicinfo['B type']) > 0\n",
      "    return pairs[idxs], target[idxs]\n",
      "\n",
      "def with_fit(clf, x, y):\n",
      "    x_test = np.linspace(x.min(), x.max(), 1000).reshape(-1, 1)\n",
      "    clf.fit(x.reshape(-1, 1), y.flatten())\n",
      "    predictions = clf.predict(x_test)\n",
      "    return x_test, predictions\n",
      "\n",
      "def fancy_plot(clf, x, y):\n",
      "    plot(x, y, 'o')\n",
      "    xlabel('A')\n",
      "    ylabel('B')\n",
      "    X_to_Y = with_fit(clf, x, y)\n",
      "    Y_to_X = with_fit(clf, y, x)\n",
      "    p1, = plot(*X_to_Y)\n",
      "    p2, = plot(*reversed(Y_to_X))\n",
      "    legend([p1, p2], [\"X->Y\", \"Y->X\"])\n",
      "    \n",
      "class PolynomialLinearRegression(object):\n",
      "    def __init__(self, degree):\n",
      "        self.degree = degree\n",
      "    def fit(self, X, y):\n",
      "        self.clf = LinearRegression()\n",
      "        new_X = np.hstack([X ** n for n in range(1, self.degree + 1)])\n",
      "        self.clf.fit(new_X, y)\n",
      "        return self\n",
      "    def predict(self, X):\n",
      "        new_X = np.hstack([X ** n for n in range(1, self.degree + 1)])\n",
      "        return self.clf.predict(new_X)\n",
      "    \n",
      "numerical, categorical, binary = \"Numerical\", \"Categorical\", \"Binary\"\n",
      "types = (numerical, categorical, binary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pairs.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publicinfo.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publicinfo['A type'].value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "publicinfo['B type'].value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CC = type_finder(categorical, categorical)\n",
      "NC = type_finder(numerical, categorical)\n",
      "CN = type_finder(categorical, numerical)\n",
      "NN = type_finder(numerical, numerical)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type_matrix = np.array([[len(type_finder(t1, t2)[0]) for t1 in types] for t2 in types])\n",
      "pd.DataFrame(type_matrix, index=types, columns=types)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i  in range(6):\n",
      "    plot_pair(i, NN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tangent: My Philosophy\n",
      "------------------\n",
      "\n",
      "### The Ultimate Goal: Automatic Feature Creation\n",
      "+ #### The data scientist is often the bottleneck\n",
      "+ #### Reproducibility\n",
      "+ #### Applicability to a wider range of problems\n",
      "\n",
      "\n",
      "### It was also a great fit for the problem (since the variables don't have context).\n",
      "\n",
      "### The results:\n",
      "+ #### No visualization.\n",
      "+ #### No hand-tuned features.\n",
      "+ #### No human-driven feedback loop.\n",
      "+ #### No state of the art features.\n",
      "\n",
      "### Doing the above things would probably improve the score."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feature Creation\n",
      "----------------\n",
      "\n",
      "#### Problem:\n",
      "+ How do we get features?\n",
      "+ We have a matrix of pairs of non-uniform length!\n",
      "+ We want a 2-D matrix to stand on the shoulders of the existing ML algorithms.\n",
      "+ How do we not lose local information?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, y = plot_pair(0, NN)  # Let's focus on numerical-numerical\n",
      "fancy_plot(LinearRegression(), x, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Solution:\n",
      "+ Compute metrics between predicted and true.\n",
      "+ Because we don't want to tune the metrics, let's use all of them!\n",
      "    + various distance metrics\n",
      "    + various statistical metrics\n",
      "    + various machine learning metrics\n",
      "\n",
      "#### Other solutions:\n",
      "+ Analyzing the images of the plots.\n",
      "+ Aggregate statistics: entropy, mean, variance, etc. (I used some of these too!)\n",
      "+ State of the art features\n",
      "\n",
      "#### Possible Improvement:\n",
      "+ Finding which metrics were most effective, and using more of those.\n",
      "+ Use cross-validated predictions instead of in-sample predictions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What we're at:\n",
      "```python\n",
      "for metric in METRICS:\n",
      "    B_to_A = metric(true_A, fit_A)\n",
      "    A_to_B = metric(true_B, fit_B)\n",
      "    features.append(B_to_A)\n",
      "    features.append(A_to_B)\n",
      "    features.append(A_to_B - B_to_A)  # Why not? Insert more of these...\n",
      "    \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Problem:\n",
      "+ How do we get a fit?\n",
      "+ Which technique would we use to be most applicable?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, y = plot_pair(4, NN)  # Note to self: show 0, then 4, then 5\n",
      "fancy_plot(GradientBoostingRegressor(max_depth=3), x, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Solution:\n",
      "+ Key insight: machine learning is (to some extent) curve fitting!\n",
      "+ Instead of choosing a technique, let's have the machine learning model pick the good ones later (:\n",
      "\n",
      "#### Other solutions:\n",
      "+ Rolling Averages\n",
      "+ Lots of different polynomials\n",
      "\n",
      "#### Possible Improvement:\n",
      "+ Finding which models were most effective, and using more of those.\n",
      "+ Use a less limited subset of models (I didn't use the smoother models because they take much longer...)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What we're at:\n",
      "```python\n",
      "for model in MODELS:\n",
      "    fit_A = model.get_fit(true_B, true_A)\n",
      "    fit_B = model.get_fit(true_A, true_B)\n",
      "    for metric in METRICS:\n",
      "        # get metrics for each feature\n",
      "        ...\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Problem:\n",
      "+ All of the above (somewhat) breaks down when we take categorical variables into account!\n",
      "+ Where do binary variables fall?\n",
      "+ If we treat them as separate problems, where do we draw the line?\n",
      "+ Can we afford to deal with much less data?\n",
      "+ Can we treat them all the same without losing too much information?\n",
      "+ Above, we used regression models to determine goodness of fit! Should we use classification models for categorical variables?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, y = plot_pair(1, CN)\n",
      "fancy_plot(GradientBoostingRegressor(max_depth=2), x, y)\n",
      "# The model assumes numeric input!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ohe_x = OneHotEncoder().fit_transform(x.reshape(-1, 1))\n",
      "sixth_col = ohe_x[:, 5].todense().A.flatten()\n",
      "fancy_plot(GradientBoostingRegressor(max_depth=2), sixth_col, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Solution:\n",
      "+ Treat binary as either categorical or numerical.\n",
      "+ For each observation (pair of variables), create separate features for:\n",
      "    + NN, NC, CN, CC\n",
      "+ To convert numerical to categorical:\n",
      "    + K-means clustering using the gap statistic to compute optimal clusters\n",
      "    + Binning\n",
      "+ To convert categorical to numerical:\n",
      "    + Randomized PCA down to one dimension\n",
      "    + One-hot encoding\n",
      "+ To combine metrics for multiple dimensions (categorical), run the previous algorithm on each column, then aggregate the results (mean, min, max, etc.) so that the features are comparable to ones with different dimensionality.\n",
      "    \n",
      "    ```python\n",
      "    # Assume A, B are defined, and without loss of generality, A is categorical\n",
      "    ohe_A = one_hot_encoding(A)\n",
      "    aggregate_features = []\n",
      "    for col in columns(A):\n",
      "        features = []\n",
      "        for model in MODELS:\n",
      "            # create features as above\n",
      "            ...\n",
      "            features.append(...)\n",
      "        aggregate_features.append(features)\n",
      "    grouped_by_feature = zip(*aggregate_features)\n",
      "    for aggregator in AGGREGATORS:  # (mean, min, max, etc.)\n",
      "        for feature in grouped_by_feature:\n",
      "            final_features.append(aggregator(feature))\n",
      "    ```\n",
      "+ Note: I did the looping and aggregation twice when both variables are categorical.\n",
      "\n",
      "#### Other solutions:\n",
      "+ Only binning\n",
      "+ Ignoring this problem and treating categorical as numerical (!!!)\n",
      "\n",
      "#### Possible Improvement:\n",
      "+ Different feature extraction methods (ICA, more clustering, etc...)\n",
      "+ Increasing the dimensionality instead of reducing it (so that we more accurately compare apples-to-apples)\n",
      "+ Different ways of aggregation in the multidimensional case"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What we're at:\n",
      "```python\n",
      "features = []\n",
      "for (A, B) in pairs:\n",
      "    pair_features = []\n",
      "    for A_type in {numerical, categorical}:  # the type to convert TO, not the actual type\n",
      "        for B_type in {numerical, categorical}:\n",
      "            for A2 in convert(A, A_type):  # apply different algorithms to convert\n",
      "                for B2 in convert(B, B_type):\n",
      "                    for column in columns(A2):\n",
      "                        for column in columns(B2):\n",
      "                            features_B\n",
      "                            for model in MODELS:\n",
      "                                # get features here\n",
      "                                ...\n",
      "                        ...\n",
      "                        # these need to be done even for 1-D input so that we can make comparisons\n",
      "                        # to when the input is not 1-D\n",
      "                        for aggregator in AGGREGATORS:\n",
      "                            ...\n",
      "                    ...\n",
      "                    # ditto here\n",
      "                    for aggregator in AGGREGATORS:\n",
      "                        ...  # append to pair_features somewhere in here\n",
      "    features.append(pair_features)\n",
      "\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Feature Selection\n",
      "----\n",
      "\n",
      "#### Problem:\n",
      "+ We have a lot of features!\n",
      "+ They're possibly redundant and poor!\n",
      "+ They could possibly hurt our accuracy (curse of dimensionality)\n",
      "\n",
      "#### (These parts are pretty normal, so I'm skipping the code!)\n",
      "\n",
      "#### Solution:\n",
      "+ Genetic algorithm:\n",
      "    + A feature bitmask maps on to genes well\n",
      "    + Meaningful cross-over\n",
      "    + Can explore large spaces quickly and better than random\n",
      "    + Very easy to code\n",
      "+ Brought features from ~9000 to ~4500\n",
      "+ Turns out this wasn't too helpful\n",
      "\n",
      "#### Other solutions:\n",
      "+ Since others' features were hand-tuned, they were also hand selected (:\n",
      "+ Things I considered and reasons they weren't used:\n",
      "    + Gaussian process sequential model-based optimization: too slow\n",
      "    + Information theoretic feature selection: not tuned to the model, performed poorly\n",
      "    + Recursive feature elimination: doesn't take into account complex relationships, computationally intensive\n",
      "    + Model specific (such as L1 regularization or decision tree feature importance): importance is extracted from the training set\n",
      "\n",
      "#### Possible Improvement:\n",
      "+ Having feature value automatically feedback into feature creation\n",
      "+ Sequential model-based optimization with random forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model Optimization\n",
      "----\n",
      "\n",
      "#### Problem:\n",
      "+ What do we do with our features?\n",
      "+ Which model(s) do we use?\n",
      "+ What hyperparameters to use?\n",
      "\n",
      "```python\n",
      "clf = GradientBoostingRegressor(\n",
      "        loss='huber',\n",
      "        n_estimators=5000,\n",
      "        random_state=1,  # always seed your random state\n",
      "        min_samples_split=2,\n",
      "        min_samples_leaf=1,\n",
      "        subsample=1.0,\n",
      "        max_features=686,\n",
      "        alpha=0.995355212043,\n",
      "        max_depth=10,\n",
      "        learning_rate=np.exp(-4.09679792914)\n",
      "    )\n",
      "```\n",
      "\n",
      "#### Solution:\n",
      "+ Gradient boosted trees\n",
      "    + Scales well with a lot of features\n",
      "    + Pretty fast\n",
      "    + Generally has amazing performance (especially with non-uniform features)\n",
      "+ Bayesian hyperparameter optimization using Gaussian Processes (see: Spearmint)\n",
      "\n",
      "#### Other solutions:\n",
      "+ Ensembled tree-based models (GBMs as well, mostly)\n",
      "+ Some neural networks\n",
      "\n",
      "#### Possible Improvement:\n",
      "+ Trying more models\n",
      "+ Ranking specific models\n",
      "+ Ensembling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Questions?\n",
      "-------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import Image\n",
      "Image('leaderboard.png')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}